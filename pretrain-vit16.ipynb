{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1376a53",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-22T08:18:51.494345Z",
     "iopub.status.busy": "2025-03-22T08:18:51.494107Z",
     "iopub.status.idle": "2025-03-22T08:18:51.615455Z",
     "shell.execute_reply": "2025-03-22T08:18:51.614634Z"
    },
    "papermill": {
     "duration": 0.128954,
     "end_time": "2025-03-22T08:18:51.616879",
     "exception": false,
     "start_time": "2025-03-22T08:18:51.487925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d410ce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:18:51.629403Z",
     "iopub.status.busy": "2025-03-22T08:18:51.629131Z",
     "iopub.status.idle": "2025-03-22T08:18:54.960863Z",
     "shell.execute_reply": "2025-03-22T08:18:54.960004Z"
    },
    "papermill": {
     "duration": 3.340684,
     "end_time": "2025-03-22T08:18:54.962830",
     "exception": false,
     "start_time": "2025-03-22T08:18:51.622146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93cdda6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:18:54.975945Z",
     "iopub.status.busy": "2025-03-22T08:18:54.975579Z",
     "iopub.status.idle": "2025-03-22T08:18:59.157058Z",
     "shell.execute_reply": "2025-03-22T08:18:59.156078Z"
    },
    "papermill": {
     "duration": 4.189362,
     "end_time": "2025-03-22T08:18:59.158755",
     "exception": false,
     "start_time": "2025-03-22T08:18:54.969393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815e94b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:18:59.169937Z",
     "iopub.status.busy": "2025-03-22T08:18:59.169585Z",
     "iopub.status.idle": "2025-03-22T08:18:59.243908Z",
     "shell.execute_reply": "2025-03-22T08:18:59.243040Z"
    },
    "papermill": {
     "duration": 0.081269,
     "end_time": "2025-03-22T08:18:59.245346",
     "exception": false,
     "start_time": "2025-03-22T08:18:59.164077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f813d281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:18:59.256597Z",
     "iopub.status.busy": "2025-03-22T08:18:59.256341Z",
     "iopub.status.idle": "2025-03-22T08:19:26.064798Z",
     "shell.execute_reply": "2025-03-22T08:19:26.063500Z"
    },
    "papermill": {
     "duration": 26.815708,
     "end_time": "2025-03-22T08:19:26.066396",
     "exception": false,
     "start_time": "2025-03-22T08:18:59.250688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thop\r\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.12.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\r\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: thop\r\n",
      "Successfully installed thop-0.1.1.post2209072238\r\n",
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\r\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\r\n",
      "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\r\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\r\n",
      "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\r\n",
      "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\r\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.11.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.29.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install thop\n",
    "!pip install torchsummary\n",
    "!pip install albumentations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from shutil import copy, rmtree\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d58f369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:19:26.078908Z",
     "iopub.status.busy": "2025-03-22T08:19:26.078418Z",
     "iopub.status.idle": "2025-03-22T08:19:26.121654Z",
     "shell.execute_reply": "2025-03-22T08:19:26.120808Z"
    },
    "papermill": {
     "duration": 0.050701,
     "end_time": "2025-03-22T08:19:26.123078",
     "exception": false,
     "start_time": "2025-03-22T08:19:26.072377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lesion_id      image_id   dx dx_type   age   sex localization\n",
      "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n",
      "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n",
      "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n",
      "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n",
      "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "data_pd = pd.read_csv('/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\n",
    "print(data_pd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72eb4574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:19:26.134866Z",
     "iopub.status.busy": "2025-03-22T08:19:26.134632Z",
     "iopub.status.idle": "2025-03-22T08:19:26.163410Z",
     "shell.execute_reply": "2025-03-22T08:19:26.162661Z"
    },
    "papermill": {
     "duration": 0.036017,
     "end_time": "2025-03-22T08:19:26.164638",
     "exception": false,
     "start_time": "2025-03-22T08:19:26.128621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lesion_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HAM_0000000</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAM_0000001</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAM_0000002</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAM_0000003</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAM_0000004</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_id  dx  dx_type  age  sex  localization\n",
       "lesion_id                                                 \n",
       "HAM_0000000         2   2        2    2    2             2\n",
       "HAM_0000001         1   1        1    1    1             1\n",
       "HAM_0000002         3   3        3    3    3             3\n",
       "HAM_0000003         1   1        1    1    1             1\n",
       "HAM_0000004         1   1        1    1    1             1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count = data_pd.groupby('lesion_id').count()\n",
    "df_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7626a06f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:19:26.176520Z",
     "iopub.status.busy": "2025-03-22T08:19:26.176299Z",
     "iopub.status.idle": "2025-03-22T08:19:26.182367Z",
     "shell.execute_reply": "2025-03-22T08:19:26.181381Z"
    },
    "papermill": {
     "duration": 0.013536,
     "end_time": "2025-03-22T08:19:26.183824",
     "exception": false,
     "start_time": "2025-03-22T08:19:26.170288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_count = df_count[df_count['dx'] == 1]\n",
    "df_count.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af248f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:19:26.195686Z",
     "iopub.status.busy": "2025-03-22T08:19:26.195471Z",
     "iopub.status.idle": "2025-03-22T08:19:26.198751Z",
     "shell.execute_reply": "2025-03-22T08:19:26.198131Z"
    },
    "papermill": {
     "duration": 0.010624,
     "end_time": "2025-03-22T08:19:26.200056",
     "exception": false,
     "start_time": "2025-03-22T08:19:26.189432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def duplicates(x):\n",
    "    unique = set(df_count['lesion_id'])\n",
    "    if x in unique:\n",
    "        return 'no' \n",
    "    else:\n",
    "        return 'duplicates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb5d392d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:19:26.211781Z",
     "iopub.status.busy": "2025-03-22T08:19:26.211566Z",
     "iopub.status.idle": "2025-03-22T08:19:32.765232Z",
     "shell.execute_reply": "2025-03-22T08:19:32.764418Z"
    },
    "papermill": {
     "duration": 6.561202,
     "end_time": "2025-03-22T08:19:32.766685",
     "exception": false,
     "start_time": "2025-03-22T08:19:26.205483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "      <td>duplicates</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   dx dx_type   age   sex localization  \\\n",
       "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n",
       "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n",
       "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n",
       "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n",
       "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n",
       "\n",
       "  is_duplicate  \n",
       "0   duplicates  \n",
       "1   duplicates  \n",
       "2   duplicates  \n",
       "3   duplicates  \n",
       "4   duplicates  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd['is_duplicate'] = data_pd['lesion_id'].apply(duplicates)\n",
    "data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc3b64d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:19:32.779685Z",
     "iopub.status.busy": "2025-03-22T08:19:32.779432Z",
     "iopub.status.idle": "2025-03-22T08:19:32.787616Z",
     "shell.execute_reply": "2025-03-22T08:19:32.786733Z"
    },
    "papermill": {
     "duration": 0.016078,
     "end_time": "2025-03-22T08:19:32.788909",
     "exception": false,
     "start_time": "2025-03-22T08:19:32.772831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_count = data_pd[data_pd['is_duplicate'] == 'no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "819d8fa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:19:32.801116Z",
     "iopub.status.busy": "2025-03-22T08:19:32.800860Z",
     "iopub.status.idle": "2025-03-22T08:21:27.103321Z",
     "shell.execute_reply": "2025-03-22T08:21:27.102326Z"
    },
    "papermill": {
     "duration": 114.316813,
     "end_time": "2025-03-22T08:21:27.111385",
     "exception": false,
     "start_time": "2025-03-22T08:19:32.794572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 7235 images\n",
      "Validation set: 1277 images\n",
      "Test set: 1503 images\n"
     ]
    }
   ],
   "source": [
    "# Paths to the dataset\n",
    "image_dir_part1 = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1'\n",
    "image_dir_part2 = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2'\n",
    "csv_file = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv'\n",
    "\n",
    "# Reading the metadata CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Create the root directory where we will save train, test, and val sets\n",
    "output_dir = '/kaggle/working/HAM10000_split'\n",
    "train_dir = os.path.join(output_dir, 'train')\n",
    "val_dir = os.path.join(output_dir, 'val')\n",
    "test_dir = os.path.join(output_dir, 'test')\n",
    "\n",
    "# Create folders for train, val, and test sets\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Split data into train, test, and validation sets\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.15, stratify=df['dx'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.15, stratify=train_val_df['dx'], random_state=42)\n",
    "\n",
    "# Function to copy images to their respective folders\n",
    "def copy_images(image_list, part1_dir, part2_dir, destination_dir):\n",
    "    for _, row in image_list.iterrows():\n",
    "        file_name = f\"{row['image_id']}.jpg\"\n",
    "        src_part1 = os.path.join(part1_dir, file_name)\n",
    "        src_part2 = os.path.join(part2_dir, file_name)\n",
    "        \n",
    "        # Check if the image is in part 1 or part 2\n",
    "        if os.path.exists(src_part1):\n",
    "            src = src_part1\n",
    "        elif os.path.exists(src_part2):\n",
    "            src = src_part2\n",
    "        else:\n",
    "            print(f\"File not found: {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        dst = os.path.join(destination_dir, row['dx'])\n",
    "        os.makedirs(dst, exist_ok=True)  # Create class subfolder if it doesn't exist\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# Copy images into train, val, and test folders\n",
    "copy_images(train_df, image_dir_part1, image_dir_part2, train_dir)\n",
    "copy_images(val_df, image_dir_part1, image_dir_part2, val_dir)\n",
    "copy_images(test_df, image_dir_part1, image_dir_part2, test_dir)\n",
    "\n",
    "print(f\"Train set: {len(train_df)} images\")\n",
    "print(f\"Validation set: {len(val_df)} images\")\n",
    "print(f\"Test set: {len(test_df)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26283311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:21:27.124565Z",
     "iopub.status.busy": "2025-03-22T08:21:27.124306Z",
     "iopub.status.idle": "2025-03-22T08:28:00.275880Z",
     "shell.execute_reply": "2025-03-22T08:28:00.274872Z"
    },
    "papermill": {
     "duration": 393.165179,
     "end_time": "2025-03-22T08:28:00.282662",
     "exception": false,
     "start_time": "2025-03-22T08:21:27.117483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from shutil import copy, rmtree\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Define paths\n",
    "origin_data_path = '/kaggle/working/HAM10000_split/train'  # Update path\n",
    "\n",
    "# Get class names (subdirectories)\n",
    "data_class = [cla for cla in os.listdir(origin_data_path)\n",
    "              if os.path.isdir(os.path.join(origin_data_path, cla))]\n",
    "\n",
    "# Define augmentations (Optimized for ViT-Base-16)\n",
    "augmentations = A.Compose([\n",
    "    A.RandomResizedCrop(224, 224, scale=(0.8, 1.0)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.HueSaturationValue(p=0.2),\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Augment images for each class\n",
    "for img_class in data_class:\n",
    "    class_path = os.path.join(origin_data_path, img_class)\n",
    "    img_list = os.listdir(class_path)\n",
    "    \n",
    "    # Create temporary directory\n",
    "    aug_dir = \"aug_dir\"\n",
    "    img_dir = os.path.join(aug_dir, img_class)\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "    # Copy original images\n",
    "    for image in img_list:\n",
    "        image_path = os.path.join(class_path, image)\n",
    "        if os.path.isfile(image_path):\n",
    "            copy(image_path, os.path.join(img_dir, image))\n",
    "\n",
    "    aug_images = 8000  # Target number of augmented images\n",
    "    num_files = len(img_list)\n",
    "    extra_images_needed = aug_images - num_files\n",
    "\n",
    "    # Apply augmentation\n",
    "    for i in range(extra_images_needed):\n",
    "        img_name = img_list[i % num_files]  # Cycle through existing images\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        # Apply Albumentations augmentation\n",
    "        augmented = augmentations(image=image)['image']\n",
    "        \n",
    "        # Save augmented image\n",
    "        aug_img_name = f\"trans_{i}.jpg\"\n",
    "        aug_img_path = os.path.join(class_path, aug_img_name)\n",
    "        augmented_pil = transforms.ToPILImage()(augmented)\n",
    "        augmented_pil.save(aug_img_path)\n",
    "\n",
    "    # Remove temporary directory\n",
    "    rmtree(aug_dir)\n",
    "\n",
    "print(\"Augmentation completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c992a497",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:28:00.295605Z",
     "iopub.status.busy": "2025-03-22T08:28:00.295324Z",
     "iopub.status.idle": "2025-03-22T08:28:07.826514Z",
     "shell.execute_reply": "2025-03-22T08:28:07.825443Z"
    },
    "papermill": {
     "duration": 7.539135,
     "end_time": "2025-03-22T08:28:07.827832",
     "exception": false,
     "start_time": "2025-03-22T08:28:00.288697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[df] processing [8000/8000]\n",
      "[mel] processing [8000/8000]\n",
      "[vasc] processing [8000/8000]\n",
      "[nv] processing [8000/8000]\n",
      "[bkl] processing [8000/8000]\n",
      "[akiec] processing [8000/8000]\n",
      "[bcc] processing [8000/8000]\n",
      "processing 56000 done!\n"
     ]
    }
   ],
   "source": [
    "# detect \n",
    "total_num = 0\n",
    "for cla in data_class:\n",
    "    cla_path = os.path.join('/kaggle/working/HAM10000_split/train', cla)\n",
    "    images = os.listdir(cla_path)\n",
    "    num = len(images)\n",
    "    total_num += num\n",
    "    for index, image in enumerate(images):\n",
    " \n",
    "        print(\"\\r[{}] processing [{}/{}]\".format(cla, index+1, num), end=\"\")  # processing bar\n",
    "    # break\n",
    "    print()\n",
    "\n",
    "print(f\"processing {total_num} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83f9ecf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:28:07.939852Z",
     "iopub.status.busy": "2025-03-22T08:28:07.939372Z",
     "iopub.status.idle": "2025-03-22T08:28:08.140935Z",
     "shell.execute_reply": "2025-03-22T08:28:08.138704Z"
    },
    "papermill": {
     "duration": 0.256962,
     "end_time": "2025-03-22T08:28:08.142439",
     "exception": false,
     "start_time": "2025-03-22T08:28:07.885477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[df] processing [17/17]\n",
      "[mel] processing [167/167]\n",
      "[vasc] processing [22/22]\n",
      "[nv] processing [1006/1006]\n",
      "[bkl] processing [165/165]\n",
      "[akiec] processing [49/49]\n",
      "[bcc] processing [77/77]\n",
      "processing 1503 done!\n"
     ]
    }
   ],
   "source": [
    "# detect \n",
    "total_num = 0\n",
    "for cla in data_class:\n",
    "    cla_path = os.path.join('/kaggle/working/HAM10000_split/test', cla)\n",
    "    images = os.listdir(cla_path)\n",
    "    num = len(images)\n",
    "    total_num += num\n",
    "    for index, image in enumerate(images):\n",
    " \n",
    "        print(\"\\r[{}] processing [{}/{}]\".format(cla, index+1, num), end=\"\")  # processing bar\n",
    "    # break\n",
    "    print()\n",
    "\n",
    "print(f\"processing {total_num} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f001c408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:28:08.252477Z",
     "iopub.status.busy": "2025-03-22T08:28:08.251944Z",
     "iopub.status.idle": "2025-03-22T08:28:08.256770Z",
     "shell.execute_reply": "2025-03-22T08:28:08.255884Z"
    },
    "papermill": {
     "duration": 0.061158,
     "end_time": "2025-03-22T08:28:08.258092",
     "exception": false,
     "start_time": "2025-03-22T08:28:08.196934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f47eb7f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:28:08.367288Z",
     "iopub.status.busy": "2025-03-22T08:28:08.366940Z",
     "iopub.status.idle": "2025-03-22T08:28:08.375618Z",
     "shell.execute_reply": "2025-03-22T08:28:08.374726Z"
    },
    "papermill": {
     "duration": 0.063884,
     "end_time": "2025-03-22T08:28:08.377001",
     "exception": false,
     "start_time": "2025-03-22T08:28:08.313117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "set_seeds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "679cbd46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:28:08.481758Z",
     "iopub.status.busy": "2025-03-22T08:28:08.481298Z",
     "iopub.status.idle": "2025-03-22T08:28:13.283007Z",
     "shell.execute_reply": "2025-03-22T08:28:13.282039Z"
    },
    "papermill": {
     "duration": 4.855521,
     "end_time": "2025-03-22T08:28:13.284607",
     "exception": false,
     "start_time": "2025-03-22T08:28:08.429086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
      "100%|██████████| 330M/330M [00:01<00:00, 181MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 7]              768                  Partial\n",
       "├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 14, 14]    (590,592)            False\n",
       "├─Encoder (encoder)                                          [32, 197, 768]       [32, 197, 768]       151,296              False\n",
       "│    └─Dropout (dropout)                                     [32, 197, 768]       [32, 197, 768]       --                   --\n",
       "│    └─Sequential (layers)                                   [32, 197, 768]       [32, 197, 768]       --                   False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    └─LayerNorm (ln)                                        [32, 197, 768]       [32, 197, 768]       (1,536)              False\n",
       "├─Linear (heads)                                             [32, 768]            [32, 7]              5,383                True\n",
       "============================================================================================================================================\n",
       "Total params: 85,804,039\n",
       "Trainable params: 5,383\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (G): 5.52\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3330.74\n",
       "Params size (MB): 229.21\n",
       "Estimated Total Size (MB): 3579.22\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Get pretrained weights for ViT-Base\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT \n",
    "\n",
    "# 2. Setup a ViT model instance with pretrained weights\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# 3. Freeze the base parameters\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# 4. Change the classifier head for the HAM10000 dataset (7 classes)\n",
    "class_names = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']  # HAM10000 dataset classes\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seeds()\n",
    "\n",
    "# Modify the head to fit the 7 classes\n",
    "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
    "\n",
    "# Print model summary\n",
    "summary(model=pretrained_vit, \n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c782807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:28:13.406533Z",
     "iopub.status.busy": "2025-03-22T08:28:13.406244Z",
     "iopub.status.idle": "2025-03-22T08:28:13.409843Z",
     "shell.execute_reply": "2025-03-22T08:28:13.408943Z"
    },
    "papermill": {
     "duration": 0.064822,
     "end_time": "2025-03-22T08:28:13.411332",
     "exception": false,
     "start_time": "2025-03-22T08:28:13.346510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = '/kaggle/working/HAM10000_split/train'\n",
    "test_dir = '/kaggle/working/HAM10000_split/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5026ef47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:28:13.524597Z",
     "iopub.status.busy": "2025-03-22T08:28:13.524098Z",
     "iopub.status.idle": "2025-03-22T08:28:13.529119Z",
     "shell.execute_reply": "2025-03-22T08:28:13.528176Z"
    },
    "papermill": {
     "duration": 0.062906,
     "end_time": "2025-03-22T08:28:13.530487",
     "exception": false,
     "start_time": "2025-03-22T08:28:13.467581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get automatic transforms from pretrained ViT weights\n",
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "print(pretrained_vit_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8bc9ccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:28:13.641940Z",
     "iopub.status.busy": "2025-03-22T08:28:13.641430Z",
     "iopub.status.idle": "2025-03-22T08:28:13.797343Z",
     "shell.execute_reply": "2025-03-22T08:28:13.796416Z"
    },
    "papermill": {
     "duration": 0.213257,
     "end_time": "2025-03-22T08:28:13.798691",
     "exception": false,
     "start_time": "2025-03-22T08:28:13.585434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
      "Training dataloader size: 875\n",
      "Testing dataloader size: 24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "\n",
    "    # Use ImageFolder to create dataset(s)\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    # Turn images into data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names\n",
    "\n",
    "\n",
    "# Update path for your Kaggle environment\n",
    "train_dir = '/kaggle/working/HAM10000_split/train'  # Adjust path based on your dataset location\n",
    "test_dir = '/kaggle/working/HAM10000_split/test'    # Adjust path based on your dataset location\n",
    "\n",
    "# Define transformations (using ViT's pretrained weights transform)\n",
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "\n",
    "# Setup dataloaders for ViT with the correct transformations\n",
    "train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=pretrained_vit_transforms,\n",
    "    batch_size=64  # Adjust the batch size as per your setup\n",
    ")\n",
    "\n",
    "# Optional: Print out class names and dataloader sizes\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"Training dataloader size: {len(train_dataloader_pretrained)}\")\n",
    "print(f\"Testing dataloader size: {len(test_dataloader_pretrained)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaa8b2f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:28:13.915852Z",
     "iopub.status.busy": "2025-03-22T08:28:13.915514Z",
     "iopub.status.idle": "2025-03-22T08:28:13.927731Z",
     "shell.execute_reply": "2025-03-22T08:28:13.927021Z"
    },
    "papermill": {
     "duration": 0.072895,
     "end_time": "2025-03-22T08:28:13.929133",
     "exception": false,
     "start_time": "2025-03-22T08:28:13.856238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# 1. Train Step\n",
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Trains a PyTorch model for a single epoch.\"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    train_loss, train_acc = 0, 0  # Initialize loss and accuracy\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)  # Send data to device\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n",
    "\n",
    "    # Return average loss and accuracy for the epoch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "# 2. Test Step\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Tests a PyTorch model for a single epoch.\"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss, test_acc = 0, 0  # Initialize loss and accuracy\n",
    "\n",
    "    # Disable gradient calculations for inference\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)  # Send data to device\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item() / len(test_pred_labels))\n",
    "\n",
    "    # Return average loss and accuracy for the epoch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# 3. Train and Test Loop\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\"\"\"\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    model.to(device)  # Send model to target device (GPU/CPU)\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Training step\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "        \n",
    "        # Testing step\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "\n",
    "        # Print metrics for the current epoch\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Store results for each epoch\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46e34971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T08:28:14.051124Z",
     "iopub.status.busy": "2025-03-22T08:28:14.050730Z",
     "iopub.status.idle": "2025-03-22T15:57:18.297676Z",
     "shell.execute_reply": "2025-03-22T15:57:18.296577Z"
    },
    "papermill": {
     "duration": 26944.367038,
     "end_time": "2025-03-22T15:57:18.356198",
     "exception": false,
     "start_time": "2025-03-22T08:28:13.989160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336b8802f8c24f519c8f824c9c64c5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0096 | train_acc: 0.6360 | test_loss: 0.6706 | test_acc: 0.7527\n",
      "Epoch: 2 | train_loss: 0.8168 | train_acc: 0.7065 | test_loss: 0.6348 | test_acc: 0.7769\n",
      "Epoch: 3 | train_loss: 0.7583 | train_acc: 0.7275 | test_loss: 0.6406 | test_acc: 0.7796\n",
      "Epoch: 4 | train_loss: 0.7255 | train_acc: 0.7384 | test_loss: 0.6174 | test_acc: 0.7841\n",
      "Epoch: 5 | train_loss: 0.7028 | train_acc: 0.7486 | test_loss: 0.6200 | test_acc: 0.7959\n",
      "Epoch: 6 | train_loss: 0.6884 | train_acc: 0.7511 | test_loss: 0.6263 | test_acc: 0.7822\n",
      "Epoch: 7 | train_loss: 0.6751 | train_acc: 0.7575 | test_loss: 0.6189 | test_acc: 0.7958\n",
      "Epoch: 8 | train_loss: 0.6664 | train_acc: 0.7599 | test_loss: 0.6129 | test_acc: 0.7789\n",
      "Epoch: 9 | train_loss: 0.6583 | train_acc: 0.7631 | test_loss: 0.6011 | test_acc: 0.7906\n",
      "Epoch: 10 | train_loss: 0.6527 | train_acc: 0.7651 | test_loss: 0.6120 | test_acc: 0.7855\n",
      "Epoch: 11 | train_loss: 0.6456 | train_acc: 0.7664 | test_loss: 0.6191 | test_acc: 0.7932\n",
      "Epoch: 12 | train_loss: 0.6413 | train_acc: 0.7695 | test_loss: 0.6187 | test_acc: 0.7808\n",
      "Epoch: 13 | train_loss: 0.6372 | train_acc: 0.7711 | test_loss: 0.6076 | test_acc: 0.7933\n",
      "Epoch: 14 | train_loss: 0.6335 | train_acc: 0.7725 | test_loss: 0.6141 | test_acc: 0.7821\n",
      "Epoch: 15 | train_loss: 0.6294 | train_acc: 0.7734 | test_loss: 0.6133 | test_acc: 0.7834\n",
      "Epoch: 16 | train_loss: 0.6277 | train_acc: 0.7742 | test_loss: 0.6125 | test_acc: 0.7932\n",
      "Epoch: 17 | train_loss: 0.6257 | train_acc: 0.7750 | test_loss: 0.6119 | test_acc: 0.7809\n",
      "Epoch: 18 | train_loss: 0.6227 | train_acc: 0.7752 | test_loss: 0.6109 | test_acc: 0.7925\n",
      "Epoch: 19 | train_loss: 0.6210 | train_acc: 0.7753 | test_loss: 0.6078 | test_acc: 0.7868\n",
      "Epoch: 20 | train_loss: 0.6196 | train_acc: 0.7756 | test_loss: 0.6150 | test_acc: 0.7997\n",
      "Epoch: 21 | train_loss: 0.6169 | train_acc: 0.7786 | test_loss: 0.6141 | test_acc: 0.7958\n",
      "Epoch: 22 | train_loss: 0.6164 | train_acc: 0.7769 | test_loss: 0.6174 | test_acc: 0.7958\n",
      "Epoch: 23 | train_loss: 0.6133 | train_acc: 0.7776 | test_loss: 0.6198 | test_acc: 0.7835\n",
      "Epoch: 24 | train_loss: 0.6131 | train_acc: 0.7775 | test_loss: 0.6147 | test_acc: 0.7984\n",
      "Epoch: 25 | train_loss: 0.6114 | train_acc: 0.7802 | test_loss: 0.6213 | test_acc: 0.7822\n",
      "Epoch: 26 | train_loss: 0.6112 | train_acc: 0.7803 | test_loss: 0.6357 | test_acc: 0.7964\n",
      "Epoch: 27 | train_loss: 0.6106 | train_acc: 0.7784 | test_loss: 0.6258 | test_acc: 0.7887\n",
      "Epoch: 28 | train_loss: 0.6099 | train_acc: 0.7790 | test_loss: 0.6300 | test_acc: 0.7835\n",
      "Epoch: 29 | train_loss: 0.6067 | train_acc: 0.7805 | test_loss: 0.6280 | test_acc: 0.7828\n",
      "Epoch: 30 | train_loss: 0.6067 | train_acc: 0.7809 | test_loss: 0.6212 | test_acc: 0.8004\n"
     ]
    }
   ],
   "source": [
    "# Create optimizer and loss function\n",
    "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "pretrained_vit_results = train(\n",
    "    model=pretrained_vit,\n",
    "    train_dataloader=train_dataloader_pretrained,\n",
    "    test_dataloader=test_dataloader_pretrained,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=30,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "319ac1b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T15:57:18.467189Z",
     "iopub.status.busy": "2025-03-22T15:57:18.466823Z",
     "iopub.status.idle": "2025-03-22T15:57:18.919103Z",
     "shell.execute_reply": "2025-03-22T15:57:18.918329Z"
    },
    "papermill": {
     "duration": 0.510525,
     "end_time": "2025-03-22T15:57:18.920837",
     "exception": false,
     "start_time": "2025-03-22T15:57:18.410312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define model save path\n",
    "model_save_path = \"pretrained_vit.pth\"\n",
    "\n",
    "# Save the trained model's state_dict\n",
    "torch.save(pretrained_vit.state_dict(), model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "920bf92e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T15:57:19.029382Z",
     "iopub.status.busy": "2025-03-22T15:57:19.029091Z",
     "iopub.status.idle": "2025-03-22T15:57:19.313644Z",
     "shell.execute_reply": "2025-03-22T15:57:19.312795Z"
    },
    "papermill": {
     "duration": 0.339837,
     "end_time": "2025-03-22T15:57:19.315195",
     "exception": false,
     "start_time": "2025-03-22T15:57:18.975358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-7bc92fc931b5>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "loaded_model = pretrained_vit  # Reinitialize model architecture\n",
    "\n",
    "# Load the saved state dictionary\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "\n",
    "# Move model to the correct device\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8bff7",
   "metadata": {
    "papermill": {
     "duration": 0.054487,
     "end_time": "2025-03-22T15:57:19.425035",
     "exception": false,
     "start_time": "2025-03-22T15:57:19.370548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27514.010387,
   "end_time": "2025-03-22T15:57:22.825230",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-22T08:18:48.814843",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "276c1cb5b26040ebb19943bc14eb1bfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "32c14e0cc56c48fa9dc4f7841fc38168": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "336b8802f8c24f519c8f824c9c64c5df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e57f575bb6884ea9b9a3dfcb1ed9c3b4",
        "IPY_MODEL_948b7cac0f1c443ea7cdbc24aad7caab",
        "IPY_MODEL_ebd25a2a43d845489644c9cd7cfbfc3a"
       ],
       "layout": "IPY_MODEL_5b91dc3292ac40298c45d41fc896556a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "59f22ffb81c44fd18aeedcce95234907": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a54564238cf4b9fb6af2bd3e6a9d6a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5b91dc3292ac40298c45d41fc896556a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68ec854c72034f7ba694bd5677e917df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "948b7cac0f1c443ea7cdbc24aad7caab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b83ccf907e3a41ae99ed850bbda8e408",
       "max": 30.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_68ec854c72034f7ba694bd5677e917df",
       "tabbable": null,
       "tooltip": null,
       "value": 30.0
      }
     },
     "b83ccf907e3a41ae99ed850bbda8e408": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e57f575bb6884ea9b9a3dfcb1ed9c3b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_59f22ffb81c44fd18aeedcce95234907",
       "placeholder": "​",
       "style": "IPY_MODEL_276c1cb5b26040ebb19943bc14eb1bfe",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "ebd25a2a43d845489644c9cd7cfbfc3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_32c14e0cc56c48fa9dc4f7841fc38168",
       "placeholder": "​",
       "style": "IPY_MODEL_5a54564238cf4b9fb6af2bd3e6a9d6a2",
       "tabbable": null,
       "tooltip": null,
       "value": " 30/30 [7:29:04&lt;00:00, 898.78s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
